
On detecting flu epidemics using social media. 
  

  What are some of the strengths of the article? Why should these strengths be considered positively? 
ANSWER: The authors consider previous weaknesses pointed out by other researchers. Namely, that using keyword matching to calculate the likelihood of an actual ILI-related event can be skewed by misclassifying non-ILI-related posts containing one of the selected keywords.  
Additionally, the authors explain the benefits and drawbacks of their model choices, namely between simple and multiple linear regression. 

    What are some of the weaknesses of the article? Why do these weaknesses hurt the article?
Selection by correlation coefficient methodology suffers here from simply taking the correlation coefficient of each tokenized word on its own, without necessary regard for other factors like parts of speech, 

    What is the overall premise of the article? Do you agree with it? Why or why not?
ANSWER: The overall premise is that it is possible to predict flu outbreaks using social media mining, natural language processing, and proper simple or multiple linear regression models. The authors make several convincing arguments. Though I have some reservations about the strength of their keyword selection process, I believe that their premise and methodology is sound, and could offer promising abilities for agencies such as the CDC to use predictive modeling in addressing public health crises. 

    In your opinion, why should this article exist? What purpose does it serve, and what questions or issues does it address?
ANSWER: This is valuable in that public health situations such as a flu outbreak can be time-sensitive; real-time reporting is crucial in detecting and responding to outbreaks before they become unmanageable. Using a publicly available corpus of 500,000 tweets collected over a 10-week period, the authors seek to model a detection classifier to predict when a flu outbreak is about to occur. 

    What is the context of this article? Why is it the logical thing to do given existing research in this area? What methodological or theoretical problems does it address?
Previous efforts to do this have identified such issues as overfitting, and have suffered from using non-real-time datasets, and have potentially suffered from overfitting in the case of using multiple linear regression. 

    What are the broader implications of this article? Whether you believe this article makes a valid conclusion or is incorrect, what does this imply for future research in this area? The true implication here is that the increasingly vast amounts of data being generated by systems (people interacting with environment and things) can be sampled and modeled to address significant problems, including social issues (crime, poverty, unemployment), public health issues (disease outbreaks) and myriad other areas that have traditionally required experiments to gather and analyze this data. Open-sourcing this type of research and data will increase the speed with which people are able to ask and answer increasingly difficult problems. 





